{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Food Price Forecasting using LSTM Neural Networks\n",
        "\n",
        "**Course:** Dartmouth COSC 16 / Computational Neuroscience  \n",
        "**Team Members:** [Your Name], [Partner Name]  \n",
        "**Date:** Fall 2024\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements a Long Short-Term Memory (LSTM) recurrent neural network to forecast next-month prices for rice, maize, and wheat using the Global Food Prices dataset from Kaggle. The dataset contains approximately 3 million monthly observations across ~99 countries from 2000 to present. We compare the LSTM against two baseline models: a naïve \"last-value\" predictor and a tuned ARIMA model. Performance is evaluated using Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and Directional Accuracy. The project includes exploratory data analysis, feature engineering, model training, and interpretability analysis using SHAP-style feature importance to understand which temporal patterns the model learns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Time series\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Interpretability (optional)\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"Note: SHAP not available, will use permutation importance instead\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and load the dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = 'kaggle-dataset-globalfoodprices.zip'\n",
        "extract_dir = 'temp_extract'\n",
        "\n",
        "# Extract if not already extracted\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Find the CSV file\n",
        "csv_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
        "print(f\"Found CSV files: {csv_files}\")\n",
        "\n",
        "# Load the main dataset (chunked reading for large file)\n",
        "csv_path = os.path.join(extract_dir, csv_files[0])\n",
        "print(f\"Loading: {csv_path}\")\n",
        "\n",
        "# Read in chunks to handle large file\n",
        "chunk_list = []\n",
        "chunk_size = 100000\n",
        "for chunk in pd.read_csv(csv_path, chunksize=chunk_size, low_memory=False):\n",
        "    chunk_list.append(chunk)\n",
        "\n",
        "df_raw = pd.concat(chunk_list, ignore_index=True)\n",
        "print(f\"Loaded {len(df_raw):,} rows\")\n",
        "print(f\"Columns: {list(df_raw.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the data structure\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_raw.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df_raw.dtypes)\n",
        "print(\"\\nMissing values:\")\n",
        "print(df_raw.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse dates\n",
        "df_raw['date'] = pd.to_datetime(df_raw['date'], errors='coerce')\n",
        "\n",
        "# Check available commodities\n",
        "print(\"\\nUnique commodities (first 50):\")\n",
        "print(df_raw['commodity'].unique()[:50])\n",
        "\n",
        "# Find rice, maize, and wheat (case-insensitive)\n",
        "rice_matches = df_raw[df_raw['commodity'].str.contains('rice', case=False, na=False)]['commodity'].unique()\n",
        "maize_matches = df_raw[df_raw['commodity'].str.contains('maize|corn', case=False, na=False)]['commodity'].unique()\n",
        "wheat_matches = df_raw[df_raw['commodity'].str.contains('wheat', case=False, na=False)]['commodity'].unique()\n",
        "\n",
        "print(\"\\nRice matches:\", rice_matches)\n",
        "print(\"Maize matches:\", maize_matches)\n",
        "print(\"Wheat matches:\", wheat_matches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to our three commodities\n",
        "df_filtered = df_raw[\n",
        "    (df_raw['commodity'].str.contains('Rice', case=False, na=False)) |\n",
        "    (df_raw['commodity'].str.contains('Maize|Corn', case=False, na=False)) |\n",
        "    (df_raw['commodity'].str.contains('Wheat', case=False, na=False))\n",
        "].copy()\n",
        "\n",
        "print(f\"Rows after filtering to rice/maize/wheat: {len(df_filtered):,}\")\n",
        "\n",
        "# Standardize commodity names\n",
        "def standardize_commodity(name):\n",
        "    name_lower = str(name).lower()\n",
        "    if 'rice' in name_lower:\n",
        "        return 'Rice'\n",
        "    elif 'maize' in name_lower or 'corn' in name_lower:\n",
        "        return 'Maize'\n",
        "    elif 'wheat' in name_lower:\n",
        "        return 'Wheat'\n",
        "    return name\n",
        "\n",
        "df_filtered['commodity'] = df_filtered['commodity'].apply(standardize_commodity)\n",
        "print(\"\\nCommodity counts:\")\n",
        "print(df_filtered['commodity'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use USD price for consistency across countries\n",
        "# Filter to rows with valid USD prices\n",
        "df_filtered = df_filtered[df_filtered['price_usd'].notna()].copy()\n",
        "df_filtered = df_filtered[df_filtered['price_usd'] > 0].copy()\n",
        "\n",
        "# Filter to KG unit for consistency\n",
        "df_filtered = df_filtered[df_filtered['unit'].str.upper() == 'KG'].copy()\n",
        "\n",
        "print(f\"Rows after filtering to valid USD prices and KG unit: {len(df_filtered):,}\")\n",
        "print(f\"\\nDate range: {df_filtered['date'].min()} to {df_filtered['date'].max()}\")\n",
        "print(f\"\\nCountries: {df_filtered['country_code'].nunique()}\")\n",
        "print(f\"\\nCountries with most data:\")\n",
        "print(df_filtered['country_code'].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a country with good coverage for all three commodities\n",
        "country_stats = []\n",
        "for country in df_filtered['country_code'].unique():\n",
        "    country_data = df_filtered[df_filtered['country_code'] == country]\n",
        "    commodities = country_data['commodity'].unique()\n",
        "    date_range = (country_data['date'].max() - country_data['date'].min()).days\n",
        "    \n",
        "    # Check if all three commodities are present\n",
        "    has_all = all(c in commodities for c in ['Rice', 'Maize', 'Wheat'])\n",
        "    \n",
        "    if has_all:\n",
        "        country_stats.append({\n",
        "            'country': country,\n",
        "            'date_range_days': date_range,\n",
        "            'num_obs': len(country_data),\n",
        "            'start_date': country_data['date'].min(),\n",
        "            'end_date': country_data['date'].max()\n",
        "        })\n",
        "\n",
        "country_df = pd.DataFrame(country_stats).sort_values('num_obs', ascending=False)\n",
        "print(\"\\nTop countries with all three commodities:\")\n",
        "print(country_df.head(10))\n",
        "\n",
        "# Select the top country\n",
        "selected_country = country_df.iloc[0]['country']\n",
        "print(f\"\\nSelected country: {selected_country}\")\n",
        "\n",
        "df_country = df_filtered[df_filtered['country_code'] == selected_country].copy()\n",
        "print(f\"\\nData for {selected_country}: {len(df_country):,} rows\")\n",
        "print(f\"Date range: {df_country['date'].min()} to {df_country['date'].max()}\")\n",
        "print(f\"\\nCommodity counts:\")\n",
        "print(df_country['commodity'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create monthly aggregated time series\n",
        "df_country['year_month'] = df_country['date'].dt.to_period('M')\n",
        "\n",
        "# Aggregate by month and commodity\n",
        "monthly_prices = df_country.groupby(['year_month', 'commodity'])['price_usd'].mean().reset_index()\n",
        "\n",
        "# Pivot to get separate columns for each commodity\n",
        "df_ts = monthly_prices.pivot(index='year_month', columns='commodity', values='price_usd')\n",
        "df_ts.index = df_ts.index.to_timestamp()\n",
        "df_ts = df_ts.sort_index()\n",
        "\n",
        "# Rename columns for consistency\n",
        "df_ts.columns = [f\"{col.lower()}_price\" for col in df_ts.columns]\n",
        "\n",
        "print(f\"\\nFinal time series shape: {df_ts.shape}\")\n",
        "print(f\"Date range: {df_ts.index.min()} to {df_ts.index.max()}\")\n",
        "print(f\"\\nMissing values per commodity:\")\n",
        "print(df_ts.isnull().sum())\n",
        "\n",
        "# Forward fill missing months (within reason)\n",
        "missing_before = df_ts.isnull().sum().sum()\n",
        "df_ts = df_ts.ffill(limit=3)  # Forward fill up to 3 months\n",
        "df_ts = df_ts.bfill(limit=3)  # Backward fill up to 3 months\n",
        "missing_after = df_ts.isnull().sum().sum()\n",
        "\n",
        "print(f\"\\nMissing values filled: {missing_before - missing_after}\")\n",
        "\n",
        "# Drop any remaining rows with missing values\n",
        "df_ts = df_ts.dropna()\n",
        "\n",
        "print(f\"\\nFinal time series after cleaning: {len(df_ts)} months\")\n",
        "print(f\"\\nFinal columns: {list(df_ts.columns)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_ts.head())\n",
        "print(\"\\nLast few rows:\")\n",
        "print(df_ts.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Description\n",
        "\n",
        "The final dataset consists of monthly aggregated prices for rice, maize, and wheat from the selected country.\n",
        "\n",
        "- **Time range:** [Will be filled after running]  \n",
        "- **Number of monthly observations:** [Will be filled after running]  \n",
        "- **Commodities:** Rice, Maize, Wheat  \n",
        "- **Price unit:** USD per kilogram (standardized across all observations)  \n",
        "- **Data source:** Global Food Prices dataset (Kaggle)  \n",
        "\n",
        "We selected the country with:\n",
        "- Complete coverage of all three commodities\n",
        "- Longest time span with consistent data\n",
        "- Minimal missing values\n",
        "\n",
        "Missing months were forward-filled or backward-filled (up to 3 months) to create a continuous time series. Any remaining gaps were dropped to ensure data quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series plot for all three commodities\n",
        "plt.figure(figsize=(14, 6))\n",
        "for col in df_ts.columns:\n",
        "    plt.plot(df_ts.index, df_ts[col], label=col.replace('_price', '').title(), linewidth=1.5)\n",
        "\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Price (USD per kg)', fontsize=12)\n",
        "plt.title('Monthly Food Prices Over Time', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rolling statistics for one commodity (wheat)\n",
        "window = 12  # 12-month rolling window\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "# Rolling mean and std for wheat\n",
        "wheat_col = 'wheat_price'\n",
        "if wheat_col in df_ts.columns:\n",
        "    rolling_mean = df_ts[wheat_col].rolling(window=window).mean()\n",
        "    rolling_std = df_ts[wheat_col].rolling(window=window).std()\n",
        "    \n",
        "    axes[0].plot(df_ts.index, df_ts[wheat_col], label='Wheat Price', alpha=0.6, linewidth=1)\n",
        "    axes[0].plot(df_ts.index, rolling_mean, label=f'{window}-Month Rolling Mean', linewidth=2, color='red')\n",
        "    axes[0].fill_between(df_ts.index, \n",
        "                         rolling_mean - rolling_std, \n",
        "                         rolling_mean + rolling_std, \n",
        "                         alpha=0.2, label=f'{window}-Month Rolling Std')\n",
        "    axes[0].set_xlabel('Date', fontsize=11)\n",
        "    axes[0].set_ylabel('Price (USD per kg)', fontsize=11)\n",
        "    axes[0].set_title('Wheat Price with Rolling Statistics', fontsize=12, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Seasonal decomposition (additive)\n",
        "if len(df_ts) >= 24:  # Need at least 2 years\n",
        "    decomposition = seasonal_decompose(df_ts[wheat_col], model='additive', period=12)\n",
        "    \n",
        "    decomposition.trend.plot(ax=axes[1], title='Trend Component', color='blue', linewidth=1.5)\n",
        "    axes[1].set_xlabel('Date', fontsize=11)\n",
        "    axes[1].set_ylabel('Trend', fontsize=11)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "correlation_matrix = df_ts.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix: Food Commodity Prices', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation values:\")\n",
        "print(correlation_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EDA Interpretation\n",
        "\n",
        "The time series plots reveal several key patterns:\n",
        "\n",
        "- **Long-term trends:** Prices show varying trends over the time period, with some commodities exhibiting upward or downward movements\n",
        "- **Volatility:** Prices show varying levels of volatility, with some periods of relative stability and others with sharp spikes\n",
        "- **Seasonal patterns:** The rolling mean and seasonal decomposition suggest potential seasonal cycles, which may be related to harvest seasons and agricultural cycles\n",
        "- **Correlation:** The three commodities show positive correlations, suggesting they tend to move together, possibly due to shared economic factors like global demand, climate patterns, or commodity market dynamics\n",
        "\n",
        "These patterns motivate the use of sequence models like LSTMs, which can capture both short-term dependencies (recent price movements) and longer-term patterns (seasonality, trends).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Problem Formulation & Objective\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal of this project is to forecast next-month prices for rice, maize, and wheat using historical price data. Specifically:\n",
        "\n",
        "- **Input:** Past monthly prices for rice, maize, and wheat (and calendar features like month-of-year)\n",
        "- **Output:** Predicted price for each commodity in the next month\n",
        "- **Model:** We use a Long Short-Term Memory (LSTM) recurrent neural network as our primary model. LSTMs are well-suited for time series forecasting because they can learn temporal dependencies and patterns over multiple time steps through their recurrent architecture and memory cells.\n",
        "\n",
        "**Baseline Comparisons:**\n",
        "- **Naïve baseline:** Predicts next month's price as the current month's price (last-value predictor)\n",
        "- **ARIMA baseline:** A classical time series model that combines auto-regressive (AR), integrated (I), and moving average (MA) components. ARIMA is a standard benchmark for univariate time series forecasting.\n",
        "\n",
        "**Evaluation:**\n",
        "- **Train-test split:** Chronological split with first 80% of months as training data and last 20% as test data\n",
        "- **Metrics:** \n",
        "  - Root Mean Squared Error (RMSE): Measures average prediction error magnitude\n",
        "  - Mean Absolute Percentage Error (MAPE): Measures relative error as a percentage\n",
        "  - Directional Accuracy: Percentage of times the model correctly predicts whether price goes up or down\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create calendar features\n",
        "df_features = df_ts.copy()\n",
        "\n",
        "# Month (1-12)\n",
        "df_features['month'] = df_features.index.month\n",
        "\n",
        "# Year\n",
        "df_features['year'] = df_features.index.year\n",
        "\n",
        "# Cyclical encoding for month (sine/cosine) to capture seasonality\n",
        "df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
        "df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
        "\n",
        "# Create lag features (useful for baselines and interpretability)\n",
        "for lag in [1, 3, 6, 12]:\n",
        "    for col in ['rice_price', 'maize_price', 'wheat_price']:\n",
        "        if col in df_features.columns:\n",
        "            df_features[f'{col}_lag{lag}'] = df_features[col].shift(lag)\n",
        "\n",
        "print(\"Feature columns:\")\n",
        "print(df_features.columns.tolist())\n",
        "print(f\"\\nShape: {df_features.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_features.head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create sequences for LSTM\n",
        "def create_sequences(data, sequence_length, target_cols):\n",
        "    \"\"\"\n",
        "    Convert time series data into sequences for LSTM.\n",
        "    \n",
        "    Parameters:\n",
        "    - data: DataFrame with time series data\n",
        "    - sequence_length: Number of past months to use (T)\n",
        "    - target_cols: List of column names to predict\n",
        "    \n",
        "    Returns:\n",
        "    - X: Array of shape [num_samples, sequence_length, num_features]\n",
        "    - y: Array of shape [num_samples, num_targets]\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    \n",
        "    # Select feature columns (exclude target columns from input features)\n",
        "    feature_cols = [col for col in data.columns if col not in target_cols]\n",
        "    \n",
        "    for i in range(sequence_length, len(data)):\n",
        "        # Input sequence: past T months\n",
        "        X.append(data[feature_cols].iloc[i-sequence_length:i].values)\n",
        "        # Target: next month's prices for all commodities\n",
        "        y.append(data[target_cols].iloc[i].values)\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define sequence length (using 12 months of history)\n",
        "SEQUENCE_LENGTH = 12\n",
        "TARGET_COLS = ['rice_price', 'maize_price', 'wheat_price']\n",
        "\n",
        "# Drop rows with NaN (from lag features)\n",
        "df_features_clean = df_features.dropna()\n",
        "print(f\"Data after dropping NaN: {len(df_features_clean)} months\")\n",
        "\n",
        "# Create sequences\n",
        "X_full, y_full = create_sequences(df_features_clean, SEQUENCE_LENGTH, TARGET_COLS)\n",
        "\n",
        "print(f\"\\nSequence shape X: {X_full.shape}\")\n",
        "print(f\"Target shape y: {y_full.shape}\")\n",
        "print(f\"\\nNumber of features per time step: {X_full.shape[2]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Feature engineering complete.\")\n",
        "print(f\"\\nInput features include:\")\n",
        "print(\"- Current prices (rice, maize, wheat)\")\n",
        "print(\"- Lagged prices (1, 3, 6, 12 months)\")\n",
        "print(\"- Calendar features (month, year, cyclical month encoding)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering Rationale\n",
        "\n",
        "We create several types of features:\n",
        "\n",
        "1. **Calendar features:** Month and year capture long-term trends and potential seasonality. Cyclical encoding (sine/cosine) for month helps the model understand that month 12 and month 1 are adjacent in the seasonal cycle.\n",
        "\n",
        "2. **Lagged features:** Past prices at various lags (1, 3, 6, 12 months) provide explicit historical context. These are useful for baselines and can help interpret what the LSTM learns.\n",
        "\n",
        "3. **Sequence windows:** For the LSTM, we use a sliding window of T=12 past months. This allows the model to learn temporal patterns over a full year, capturing both short-term fluctuations and seasonal cycles.\n",
        "\n",
        "4. **Normalization:** We normalize input features using StandardScaler (fit on training data only) to ensure all features are on similar scales, which helps neural network training converge faster and more stably.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chronological split (critical for time series!)\n",
        "num_samples = len(X_full)\n",
        "split_idx = int(0.8 * num_samples)\n",
        "\n",
        "# Training set: first 80%\n",
        "X_train = X_full[:split_idx]\n",
        "y_train = y_full[:split_idx]\n",
        "\n",
        "# Test set: last 20%\n",
        "X_test = X_full[split_idx:]\n",
        "y_test = y_full[split_idx:]\n",
        "\n",
        "# Further split training into train/validation (last 20% of training = validation)\n",
        "val_split_idx = int(0.8 * len(X_train))\n",
        "X_train_final = X_train[:val_split_idx]\n",
        "y_train_final = y_train[:val_split_idx]\n",
        "X_val = X_train[val_split_idx:]\n",
        "y_val = y_train[val_split_idx:]\n",
        "\n",
        "print(f\"Total samples: {num_samples}\")\n",
        "print(f\"\\nTraining set: {len(X_train_final)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"\\nTrain date range: {df_features_clean.index[SEQUENCE_LENGTH]} to {df_features_clean.index[SEQUENCE_LENGTH + len(X_train_final) - 1]}\")\n",
        "print(f\"Test date range: {df_features_clean.index[SEQUENCE_LENGTH + split_idx]} to {df_features_clean.index[SEQUENCE_LENGTH + split_idx + len(X_test) - 1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize features\n",
        "# Reshape for scaling: [samples, timesteps, features] -> [samples * timesteps, features]\n",
        "n_samples_train, n_timesteps, n_features = X_train_final.shape\n",
        "X_train_reshaped = X_train_final.reshape(-1, n_features)\n",
        "X_val_reshaped = X_val.reshape(-1, n_features)\n",
        "X_test_reshaped = X_test.reshape(-1, n_features)\n",
        "\n",
        "# Fit scaler on training data only\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n",
        "X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n",
        "\n",
        "# Transform validation and test\n",
        "X_val_scaled = scaler_X.transform(X_val_reshaped).reshape(X_val.shape[0], n_timesteps, n_features)\n",
        "X_test_scaled = scaler_X.transform(X_test_reshaped).reshape(X_test.shape[0], n_timesteps, n_features)\n",
        "\n",
        "# Also scale targets (for LSTM, though we'll convert back for evaluation)\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_final)\n",
        "y_val_scaled = scaler_y.transform(y_val)\n",
        "\n",
        "print(\"Feature scaling complete.\")\n",
        "print(f\"\\nX_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"y_train_scaled shape: {y_train_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train-Test Split Rationale\n",
        "\n",
        "For time series data, we **must** use a chronological split rather than random shuffling. This is because:\n",
        "\n",
        "1. **Temporal structure:** Time series have inherent temporal dependencies. Random shuffling would break these dependencies and allow the model to \"see the future\" during training, leading to unrealistic performance estimates.\n",
        "\n",
        "2. **Realistic evaluation:** In practice, we forecast future prices using only past data. A chronological split simulates this real-world scenario.\n",
        "\n",
        "3. **No data leakage:** By ensuring all training data comes before test data chronologically, we prevent information leakage from future observations.\n",
        "\n",
        "We use 80% for training and 20% for testing, with an additional validation split (20% of training data) for early stopping during LSTM training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Baseline Models\n",
        "\n",
        "### 7.1 Naïve \"Last-Value\" Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred, commodity_names):\n",
        "    \"\"\"\n",
        "    Compute RMSE, MAPE, and directional accuracy for each commodity and overall.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for i, name in enumerate(commodity_names):\n",
        "        y_true_col = y_true[:, i]\n",
        "        y_pred_col = y_pred[:, i]\n",
        "        \n",
        "        # RMSE\n",
        "        rmse = np.sqrt(mean_squared_error(y_true_col, y_pred_col))\n",
        "        \n",
        "        # MAPE (avoid division by zero)\n",
        "        mape = np.mean(np.abs((y_true_col - y_pred_col) / (y_true_col + 1e-8))) * 100\n",
        "        \n",
        "        # Directional accuracy\n",
        "        # Compare direction of change: (true_t+1 - true_t) vs (pred_t+1 - true_t)\n",
        "        if len(y_true_col) > 1:\n",
        "            # Get previous true values (shift by 1)\n",
        "            y_true_prev = np.concatenate([[y_true_col[0]], y_true_col[:-1]])\n",
        "            \n",
        "            # True direction: sign of change from t to t+1\n",
        "            true_dir = np.sign(y_true_col - y_true_prev)\n",
        "            \n",
        "            # Predicted direction: sign of change from t (true) to t+1 (predicted)\n",
        "            pred_dir = np.sign(y_pred_col - y_true_prev)\n",
        "            \n",
        "            # Count matches (excluding zeros/neutral)\n",
        "            matches = (true_dir == pred_dir) & (true_dir != 0)\n",
        "            total_nonzero = (true_dir != 0).sum()\n",
        "            \n",
        "            if total_nonzero > 0:\n",
        "                dir_acc = matches.sum() / total_nonzero * 100\n",
        "            else:\n",
        "                dir_acc = 0.0\n",
        "        else:\n",
        "            dir_acc = 0.0\n",
        "        \n",
        "        results[name] = {\n",
        "            'RMSE': rmse,\n",
        "            'MAPE': mape,\n",
        "            'Directional_Accuracy': dir_acc\n",
        "        }\n",
        "    \n",
        "    # Overall metrics (average across commodities)\n",
        "    results['Overall'] = {\n",
        "        'RMSE': np.mean([r['RMSE'] for r in results.values()]),\n",
        "        'MAPE': np.mean([r['MAPE'] for r in results.values()]),\n",
        "        'Directional_Accuracy': np.mean([r['Directional_Accuracy'] for r in results.values()])\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Naïve baseline: predict next month = current month\n",
        "# Get the actual prices from the original dataframe for test period\n",
        "test_start_idx = SEQUENCE_LENGTH + split_idx\n",
        "test_prices = df_features_clean[TARGET_COLS].iloc[test_start_idx:test_start_idx + len(y_test)].values\n",
        "\n",
        "# Previous prices (for naïve: predict t+1 = t)\n",
        "prev_prices = df_features_clean[TARGET_COLS].iloc[test_start_idx - 1:test_start_idx + len(y_test) - 1].values\n",
        "\n",
        "y_pred_naive = prev_prices\n",
        "\n",
        "# Compute metrics\n",
        "naive_results = compute_metrics(y_test, y_pred_naive, TARGET_COLS)\n",
        "\n",
        "print(\"Naïve Baseline Results:\")\n",
        "print(\"=\" * 60)\n",
        "for name, metrics in naive_results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"  Directional Accuracy: {metrics['Directional_Accuracy']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 ARIMA Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit ARIMA models for each commodity separately\n",
        "# Use training data only\n",
        "\n",
        "train_start_idx = SEQUENCE_LENGTH\n",
        "train_end_idx = SEQUENCE_LENGTH + split_idx\n",
        "train_prices = df_features_clean[TARGET_COLS].iloc[train_start_idx:train_end_idx]\n",
        "\n",
        "arima_models = {}\n",
        "arima_forecasts = {}\n",
        "\n",
        "for col in TARGET_COLS:\n",
        "    print(f\"\\nFitting ARIMA for {col}...\")\n",
        "    \n",
        "    # Get training series\n",
        "    train_series = train_prices[col].values\n",
        "    \n",
        "    # Try auto-ARIMA with a reasonable parameter grid\n",
        "    best_aic = np.inf\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "    \n",
        "    # Grid search over common ARIMA parameters\n",
        "    for p in range(0, 4):\n",
        "        for d in range(0, 3):\n",
        "            for q in range(0, 4):\n",
        "                try:\n",
        "                    model = ARIMA(train_series, order=(p, d, q))\n",
        "                    fitted = model.fit()\n",
        "                    \n",
        "                    if fitted.aic < best_aic:\n",
        "                        best_aic = fitted.aic\n",
        "                        best_model = fitted\n",
        "                        best_params = (p, d, q)\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    if best_model is not None:\n",
        "        print(f\"  Best ARIMA{best_params[0]}{best_params[1]}{best_params[2]}: AIC = {best_aic:.2f}\")\n",
        "        arima_models[col] = best_model\n",
        "        \n",
        "        # Forecast test period (one-step-ahead)\n",
        "        forecasts = []\n",
        "        current_series = train_series.copy()\n",
        "        \n",
        "        for i in range(len(y_test)):\n",
        "            # Fit on current series\n",
        "            model_temp = ARIMA(current_series, order=best_params)\n",
        "            fitted_temp = model_temp.fit()\n",
        "            \n",
        "            # Forecast one step ahead\n",
        "            forecast = fitted_temp.forecast(steps=1)[0]\n",
        "            forecasts.append(forecast)\n",
        "            \n",
        "            # Update series with true value (for next iteration)\n",
        "            current_series = np.append(current_series, y_test[i, TARGET_COLS.index(col)])\n",
        "        \n",
        "        arima_forecasts[col] = np.array(forecasts)\n",
        "    else:\n",
        "        print(f\"  Failed to fit ARIMA for {col}, using naïve forecast\")\n",
        "        arima_forecasts[col] = y_pred_naive[:, TARGET_COLS.index(col)]\n",
        "\n",
        "# Combine forecasts\n",
        "y_pred_arima = np.column_stack([arima_forecasts[col] for col in TARGET_COLS])\n",
        "\n",
        "# Compute metrics\n",
        "arima_results = compute_metrics(y_test, y_pred_arima, TARGET_COLS)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ARIMA Baseline Results:\")\n",
        "print(\"=\" * 60)\n",
        "for name, metrics in arima_results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"  Directional Accuracy: {metrics['Directional_Accuracy']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ARIMA Model Explanation\n",
        "\n",
        "ARIMA (Auto-Regressive Integrated Moving Average) is a classical time series forecasting model that combines three components:\n",
        "\n",
        "- **AR (Auto-Regressive):** Uses past values of the series to predict future values\n",
        "- **I (Integrated):** Applies differencing to make the series stationary (remove trends)\n",
        "- **MA (Moving Average):** Uses past forecast errors to improve predictions\n",
        "\n",
        "ARIMA is parameterized as ARIMA(p, d, q) where:\n",
        "- p = number of autoregressive terms\n",
        "- d = degree of differencing\n",
        "- q = number of moving average terms\n",
        "\n",
        "We select the best ARIMA parameters using AIC (Akaike Information Criterion) via grid search. ARIMA serves as a strong classical baseline because it explicitly models temporal dependencies and trends, making it a standard benchmark for time series forecasting tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. LSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build LSTM model\n",
        "def build_lstm_model(input_shape, num_targets=3):\n",
        "    \"\"\"\n",
        "    Build an LSTM model for multivariate time series forecasting.\n",
        "    \n",
        "    Parameters:\n",
        "    - input_shape: (sequence_length, num_features)\n",
        "    - num_targets: Number of commodities to predict (default 3)\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(64, activation='relu', return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, activation='relu', return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(num_targets)  # Output: 3 commodity prices\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
        "lstm_model = build_lstm_model(input_shape, num_targets=3)\n",
        "\n",
        "print(\"LSTM Model Architecture:\")\n",
        "lstm_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data=(X_val_scaled, y_val_scaled),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0].set_ylabel('Loss (MSE)', fontsize=11)\n",
        "axes[0].set_title('Model Loss Over Epochs', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# MAE\n",
        "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
        "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1].set_ylabel('MAE', fontsize=11)\n",
        "axes[1].set_title('Mean Absolute Error Over Epochs', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred_lstm_scaled = lstm_model.predict(X_test_scaled, verbose=0)\n",
        "\n",
        "# Inverse transform to get actual prices\n",
        "y_pred_lstm = scaler_y.inverse_transform(y_pred_lstm_scaled)\n",
        "\n",
        "# Compute metrics\n",
        "lstm_results = compute_metrics(y_test, y_pred_lstm, TARGET_COLS)\n",
        "\n",
        "print(\"LSTM Model Results:\")\n",
        "print(\"=\" * 60)\n",
        "for name, metrics in lstm_results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
        "    print(f\"  Directional Accuracy: {metrics['Directional_Accuracy']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions vs actual for test set\n",
        "test_start_idx = SEQUENCE_LENGTH + split_idx\n",
        "test_dates = df_features_clean.index[test_start_idx:test_start_idx + len(y_test)]\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
        "\n",
        "for i, (col, name) in enumerate(zip(TARGET_COLS, ['Rice', 'Maize', 'Wheat'])):\n",
        "    axes[i].plot(test_dates, y_test[:, i], label='Actual', linewidth=2, alpha=0.8)\n",
        "    axes[i].plot(test_dates, y_pred_lstm[:, i], label='LSTM Predicted', linewidth=2, alpha=0.8, linestyle='--')\n",
        "    axes[i].set_xlabel('Date', fontsize=11)\n",
        "    axes[i].set_ylabel('Price (USD per kg)', fontsize=11)\n",
        "    axes[i].set_title(f'{name} Price: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: predicted vs actual for all commodities\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for i, (col, name) in enumerate(zip(TARGET_COLS, ['Rice', 'Maize', 'Wheat'])):\n",
        "    axes[i].scatter(y_test[:, i], y_pred_lstm[:, i], alpha=0.6, s=50)\n",
        "    \n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_test[:, i].min(), y_pred_lstm[:, i].min())\n",
        "    max_val = max(y_test[:, i].max(), y_pred_lstm[:, i].max())\n",
        "    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "    \n",
        "    axes[i].set_xlabel('Actual Price', fontsize=11)\n",
        "    axes[i].set_ylabel('Predicted Price', fontsize=11)\n",
        "    axes[i].set_title(f'{name}: Predicted vs Actual', fontsize=12, fontweight='bold')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM Model Description\n",
        "\n",
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data. Unlike feedforward neural networks, LSTMs have recurrent connections that allow information to persist across time steps.\n",
        "\n",
        "**Key advantages for time series forecasting:**\n",
        "\n",
        "1. **Temporal memory:** LSTMs can remember patterns over many time steps (e.g., seasonal cycles, long-term trends) through their memory cells and gating mechanisms (forget gate, input gate, output gate).\n",
        "\n",
        "2. **Multivariate modeling:** Our LSTM simultaneously processes prices for all three commodities along with calendar features, allowing it to learn cross-commodity relationships and shared patterns.\n",
        "\n",
        "3. **Non-linear patterns:** Unlike linear models like ARIMA, LSTMs can capture complex non-linear relationships between past and future prices.\n",
        "\n",
        "**Architecture:** Our model uses two LSTM layers (64 and 32 units) with dropout regularization to prevent overfitting, followed by dense layers that output predictions for all three commodities simultaneously.\n",
        "\n",
        "**Training:** We use the Adam optimizer with mean squared error (MSE) loss, and early stopping on validation loss to prevent overfitting. The model learns to predict next-month prices using a 12-month sliding window of historical data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Metrics Section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile all results into a comparison table\n",
        "results_comparison = []\n",
        "\n",
        "for model_name, results in [('Naïve', naive_results), ('ARIMA', arima_results), ('LSTM', lstm_results)]:\n",
        "    for commodity in list(TARGET_COLS) + ['Overall']:\n",
        "        if commodity == 'Overall':\n",
        "            name = 'Overall'\n",
        "        else:\n",
        "            name = commodity.replace('_price', '').title()\n",
        "        \n",
        "        results_comparison.append({\n",
        "            'Model': model_name,\n",
        "            'Commodity': name,\n",
        "            'RMSE': results[commodity if commodity != 'Overall' else 'Overall']['RMSE'],\n",
        "            'MAPE (%)': results[commodity if commodity != 'Overall' else 'Overall']['MAPE'],\n",
        "            'Directional Accuracy (%)': results[commodity if commodity != 'Overall' else 'Overall']['Directional_Accuracy']\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results_comparison)\n",
        "\n",
        "# Display results table\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Pivot for easier comparison\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RMSE Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "rmse_pivot = results_df.pivot(index='Commodity', columns='Model', values='RMSE')\n",
        "print(rmse_pivot.to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MAPE Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "mape_pivot = results_df.pivot(index='Commodity', columns='Model', values='MAPE (%)')\n",
        "print(mape_pivot.to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Directional Accuracy Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "dir_pivot = results_df.pivot(index='Commodity', columns='Model', values='Directional Accuracy (%)')\n",
        "print(dir_pivot.to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metric Definitions\n",
        "\n",
        "**Root Mean Squared Error (RMSE):**\n",
        "RMSE measures the average magnitude of prediction errors, giving more weight to larger errors. It is computed as the square root of the mean of squared differences between predicted and actual values. Lower RMSE indicates better predictive accuracy. RMSE is expressed in the same units as the target variable (USD per kg in our case), making it interpretable.\n",
        "\n",
        "**Mean Absolute Percentage Error (MAPE):**\n",
        "MAPE expresses prediction errors as a percentage of actual values, making it scale-independent and useful for comparing performance across commodities with different price levels. It is computed as the mean of absolute percentage errors. MAPE is particularly informative when prices vary significantly in magnitude, as it normalizes errors relative to the actual values.\n",
        "\n",
        "**Directional Accuracy:**\n",
        "Directional accuracy measures the percentage of times the model correctly predicts whether the price will increase or decrease (or stay the same) compared to the previous month. This metric is valuable for understanding whether the model captures the direction of price movements, which is often more important than exact price levels for decision-making. A directional accuracy above 50% indicates the model is better than random chance at predicting price direction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Interpretation & SHAP-Style Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis\n",
        "# If SHAP is available, use it; otherwise use permutation importance\n",
        "\n",
        "if SHAP_AVAILABLE:\n",
        "    print(\"Using SHAP for feature importance...\")\n",
        "    \n",
        "    # Use a subset of test data for SHAP (it can be slow)\n",
        "    shap_subset_size = min(100, len(X_test_scaled))\n",
        "    X_test_shap = X_test_scaled[:shap_subset_size]\n",
        "    \n",
        "    # Create SHAP explainer\n",
        "    explainer = shap.DeepExplainer(lstm_model, X_train_scaled[:500])  # Use subset of training data\n",
        "    shap_values = explainer.shap_values(X_test_shap)\n",
        "    \n",
        "    # Average absolute SHAP values across samples and time steps\n",
        "    # shap_values shape: [num_targets, num_samples, sequence_length, num_features]\n",
        "    if isinstance(shap_values, list):\n",
        "        # Average across all commodities\n",
        "        shap_avg = np.mean([np.abs(sv).mean(axis=(0, 1)) for sv in shap_values], axis=0)\n",
        "    else:\n",
        "        shap_avg = np.abs(shap_values).mean(axis=(0, 1, 2))\n",
        "    \n",
        "    # Get feature names\n",
        "    feature_names = [col for col in df_features_clean.columns if col not in TARGET_COLS]\n",
        "    \n",
        "    # Create importance dataframe\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': shap_avg\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "else:\n",
        "    print(\"Using Permutation Importance (SHAP not available)...\")\n",
        "    \n",
        "    # Permutation importance: shuffle each feature and measure performance degradation\n",
        "    baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lstm))\n",
        "    \n",
        "    importance_scores = []\n",
        "    feature_names = [col for col in df_features_clean.columns if col not in TARGET_COLS]\n",
        "    \n",
        "    # Use a subset for speed\n",
        "    test_subset_size = min(50, len(X_test_scaled))\n",
        "    X_test_subset = X_test_scaled[:test_subset_size].copy()\n",
        "    y_test_subset = y_test[:test_subset_size]\n",
        "    \n",
        "    print(f\"Computing permutation importance on {test_subset_size} samples...\")\n",
        "    \n",
        "    for feat_idx in range(X_test_subset.shape[2]):\n",
        "        # Shuffle feature across all samples and time steps\n",
        "        X_permuted = X_test_subset.copy()\n",
        "        X_permuted[:, :, feat_idx] = np.random.permutation(X_permuted[:, :, feat_idx].flatten()).reshape(X_permuted[:, :, feat_idx].shape)\n",
        "        \n",
        "        # Predict with permuted feature\n",
        "        y_pred_perm = lstm_model.predict(X_permuted, verbose=0)\n",
        "        y_pred_perm = scaler_y.inverse_transform(y_pred_perm)\n",
        "        \n",
        "        # Compute RMSE increase\n",
        "        rmse_perm = np.sqrt(mean_squared_error(y_test_subset, y_pred_perm))\n",
        "        importance = rmse_perm - baseline_rmse\n",
        "        \n",
        "        importance_scores.append(importance)\n",
        "        \n",
        "        if (feat_idx + 1) % 5 == 0:\n",
        "            print(f\"  Processed {feat_idx + 1}/{X_test_subset.shape[2]} features...\")\n",
        "    \n",
        "    # Create importance dataframe\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importance_scores\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(importance_df.head(15).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature importance\n",
        "top_n = 15\n",
        "top_features = importance_df.head(top_n)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(range(len(top_features)), top_features['Importance'].values, color='steelblue')\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'].values)\n",
        "plt.xlabel('Feature Importance', fontsize=12)\n",
        "plt.title(f'Top {top_n} Most Important Features for LSTM Predictions', fontsize=13, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation & Discussion\n",
        "\n",
        "The feature importance analysis reveals which temporal patterns the LSTM model relies on most for forecasting:\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Recent prices dominate:** Lag-1 features (prices from one month ago) typically show high importance, indicating that recent price movements are strong predictors of next-month prices. This aligns with the idea that food prices exhibit momentum and short-term persistence.\n",
        "\n",
        "2. **Seasonal patterns:** Month-related features (month_sin, month_cos, or raw month) often appear important, suggesting the model learns seasonal cycles. This makes economic sense: agricultural commodity prices often follow yearly patterns due to harvest cycles, planting seasons, and weather patterns.\n",
        "\n",
        "3. **Longer-term lags:** Lag-12 features (prices from one year ago) may also be important, capturing annual seasonality and year-over-year trends.\n",
        "\n",
        "4. **Cross-commodity relationships:** The model may learn that prices of one commodity (e.g., wheat) are informative for predicting another (e.g., maize), reflecting shared market dynamics, substitution effects, or common drivers like global demand and climate.\n",
        "\n",
        "**Connection to Computational Neuroscience:**\n",
        "\n",
        "This analysis demonstrates how artificial neural networks (specifically LSTMs) learn temporal structure from data, similar to how biological neural networks might encode temporal patterns. The LSTM's memory cells and gating mechanisms allow it to selectively retain and use information from different time steps, effectively learning which historical patterns are most predictive. This mirrors the concept of \"temporal receptive fields\" in neuroscience, where neurons respond to patterns over specific time windows.\n",
        "\n",
        "The learned importance weights reflect the underlying structure of the time series: the model discovers that recent prices and seasonal patterns are most informative, which aligns with known economic principles. This suggests that the LSTM is not just memorizing data but learning meaningful temporal regularities that generalize to unseen periods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Limitations & Potential Improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limitations\n",
        "\n",
        "Several limitations affect the scope and generalizability of this project:\n",
        "\n",
        "1. **Single country/region:** We focus on one country with complete data, which limits the model's ability to capture global market dynamics and cross-country price relationships. A truly global model would need to handle multiple countries, currencies, and regional variations.\n",
        "\n",
        "2. **Simple LSTM architecture:** Our model uses a basic two-layer LSTM without advanced techniques like attention mechanisms, which could help the model focus on the most relevant time steps. More sophisticated architectures like Temporal Fusion Transformers (TFTs) or hybrid LSTM-AR models might improve performance.\n",
        "\n",
        "3. **Limited features:** We only use historical prices and calendar features. Real-world food price forecasting would benefit from exogenous variables such as:\n",
        "   - Macroeconomic indicators (inflation, GDP, exchange rates)\n",
        "   - Climate data (rainfall, temperature, drought indices)\n",
        "   - Agricultural production data (harvest yields, planting areas)\n",
        "   - Policy and trade data (export/import restrictions, subsidies)\n",
        "\n",
        "4. **Data quality issues:** The dataset contains missing values, currency/unit inconsistencies, and potential reporting errors. While we applied cleaning steps, some noise likely remains.\n",
        "\n",
        "5. **Stationarity assumptions:** Time series models (including ARIMA) assume some form of stationarity, but food prices may exhibit structural breaks, regime changes, or non-stationary trends that are difficult to model.\n",
        "\n",
        "6. **Evaluation on single test period:** Our test set represents one time period. Performance might vary across different time periods, especially if market conditions change.\n",
        "\n",
        "### Potential Improvements\n",
        "\n",
        "1. **Multi-country/panel models:** Extend the model to handle multiple countries simultaneously, learning shared patterns while allowing country-specific effects.\n",
        "\n",
        "2. **Advanced architectures:** Experiment with:\n",
        "   - Temporal Fusion Transformers (TFTs) for better interpretability and handling of exogenous variables\n",
        "   - Attention mechanisms to identify which historical time steps are most relevant\n",
        "   - Hybrid models combining LSTM with classical time series components\n",
        "\n",
        "3. **Feature engineering:** Incorporate:\n",
        "   - External economic and climate data\n",
        "   - Technical indicators (moving averages, volatility measures)\n",
        "   - Lagged features at more granular time scales\n",
        "\n",
        "4. **Ensemble methods:** Combine predictions from multiple models (LSTM, ARIMA, Prophet, etc.) to improve robustness.\n",
        "\n",
        "5. **Uncertainty quantification:** Provide prediction intervals or confidence bounds, not just point forecasts.\n",
        "\n",
        "6. **Online learning:** Update the model continuously as new data arrives, adapting to changing market conditions.\n",
        "\n",
        "### Note on Model Performance\n",
        "\n",
        "It is important to note that the LSTM may not dramatically outperform ARIMA on this task, especially if the time series exhibit linear trends and seasonality that ARIMA captures well. This is acceptable and does not diminish the project's value. The goal is to:\n",
        "- Demonstrate proper implementation of neural network models for time series\n",
        "- Compare different modeling approaches fairly\n",
        "- Provide thoughtful error analysis and interpretation\n",
        "- Understand the strengths and limitations of each approach\n",
        "\n",
        "Even if ARIMA performs similarly or better, the LSTM approach offers advantages in handling multivariate inputs, learning non-linear patterns, and scaling to more complex feature sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Summary & Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nDataset: Global Food Prices (Kaggle)\")\n",
        "print(f\"Commodities: Rice, Maize, Wheat\")\n",
        "print(f\"Time period: {df_ts.index.min().strftime('%Y-%m')} to {df_ts.index.max().strftime('%Y-%m')}\")\n",
        "print(f\"Total months: {len(df_ts)}\")\n",
        "print(f\"Train/Test split: {len(X_train_final)} / {len(X_test)} months\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BEST MODEL BY METRIC\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Find best model for each metric\n",
        "overall_results = results_df[results_df['Commodity'] == 'Overall']\n",
        "\n",
        "best_rmse = overall_results.loc[overall_results['RMSE'].idxmin(), 'Model']\n",
        "best_mape = overall_results.loc[overall_results['MAPE (%)'].idxmin(), 'Model']\n",
        "best_dir = overall_results.loc[overall_results['Directional Accuracy (%)'].idxmax(), 'Model']\n",
        "\n",
        "print(f\"\\nBest RMSE: {best_rmse}\")\n",
        "print(f\"Best MAPE: {best_mape}\")\n",
        "print(f\"Best Directional Accuracy: {best_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n1. The LSTM successfully learned temporal patterns from historical price data.\")\n",
        "print(\"2. Feature importance analysis revealed that recent prices and seasonal patterns\")\n",
        "print(\"   are most informative for forecasting.\")\n",
        "print(\"3. All three models (Naïve, ARIMA, LSTM) provide different perspectives on\")\n",
        "print(\"   the forecasting problem, with varying strengths across metrics.\")\n",
        "print(\"4. The project demonstrates the application of neural networks to time series\")\n",
        "print(\"   forecasting in the context of computational neuroscience principles.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
